import os
import requests
import base64
import re
from dotenv import load_dotenv
from supabase import create_client, Client
from difflib import SequenceMatcher

load_dotenv()

# Groq Llama 4 Scout Vision - MODEL M·ªöI NH·∫§T 2025
GROQ_SEARCH_IMAGE_API_KEY = os.getenv("GROQ_SEARCH_IMAGE_API_KEY")
VISION_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"  # Llama 4 Scout - Vision model m·ªõi nh·∫•t
# C√°c model kh√°c:
# - "llama-3.2-90b-vision-preview" (Vision 3.2 - 90B)
# - "llama-3.2-11b-vision-preview" (Vision 3.2 - 11B)

# Supabase
DATA_BASE_SECRET_KEY_SUPABASE = os.getenv("DATA_BASE_SECRET_KEY_SUPABASE")
DATA_BASE_URL_SUPABASE = os.getenv("DATA_BASE_URL_SUPABASE")

url = DATA_BASE_URL_SUPABASE
key = DATA_BASE_SECRET_KEY_SUPABASE
supabase: Client = create_client(url, key)


# ==================== HELPER FUNCTIONS ====================

def fetch_product_names():
    """L·∫•y danh s√°ch t√™n product t·ª´ Supabase"""
    try:
        response = supabase.table("product").select("name").execute()
        rows = response.data
        if not rows:
            print("‚ö†Ô∏è D·ªØ li·ªáu r·ªóng t·ª´ Supabase")
            return []

        names = {row["name"].strip() for row in rows if row.get("name")}
        return list(names)

    except Exception as e:
        print(f"‚ö†Ô∏è Exception fetch_product_names: {e}")
        return []


def normalize_text(text: str) -> str:
    """Chu·∫©n h√≥a text ƒë·ªÉ so s√°nh"""
    text = text.lower().strip()
    text = re.sub(r'[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', '', text)
    return text


def fuzzy_match_product(detected_text: str, products: list) -> str:
    """
    So s√°nh m·ªù ƒë·ªÉ t√¨m s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t v·ªõi multi-level matching
    """
    detected_normalized = normalize_text(detected_text)
    
    print(f"üîç ƒêang t√¨m ki·∫øm cho: '{detected_normalized}'")
    
    best_match = None
    best_score = 0.0
    
    for product in products:
        product_normalized = normalize_text(product)
        
        # Level 1: Exact match (priority cao nh·∫•t)
        if detected_normalized == product_normalized:
            print(f"  ‚úì‚úì‚úì Exact match: '{product}'")
            return product
        
        # Level 2: Substring match
        if detected_normalized in product_normalized:
            score = 0.95
            print(f"  ‚úì‚úì Substring match (detected in product): '{product}' (score: {score})")
            if score > best_score:
                best_score = score
                best_match = product
        elif product_normalized in detected_normalized:
            score = 0.90
            print(f"  ‚úì‚úì Substring match (product in detected): '{product}' (score: {score})")
            if score > best_score:
                best_score = score
                best_match = product
        
        # Level 3: Word overlap match
        detected_words = set(detected_normalized.split())
        product_words = set(product_normalized.split())
        
        if detected_words & product_words:
            common_words = detected_words & product_words
            common_ratio = len(common_words) / max(len(detected_words), len(product_words))
            
            if common_ratio > 0.5 and common_ratio > best_score:
                best_score = common_ratio
                best_match = product
                print(f"  ‚úì Word match: '{product}' | Common words: {common_words} (score: {common_ratio:.2f})")
        
        # Level 4: Fuzzy similarity
        similarity = SequenceMatcher(None, detected_normalized, product_normalized).ratio()
        if similarity > 0.65 and similarity > best_score:
            best_score = similarity
            best_match = product
            print(f"  ‚úì Fuzzy match: '{product}' (score: {similarity:.2f})")
    
    if best_match:
        print(f"‚úÖ Best match found: '{best_match}' (confidence: {best_score:.2f})")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y match cho '{detected_text}'")
    
    return best_match


def prepare_image_data(image_data: str):
    """
    Chu·∫©n b·ªã image data cho Groq API (base64)
    Returns: (base64_string, mime_type) ho·∫∑c (None, None)
    """
    try:
        # N·∫øu l√† URL
        if image_data.startswith('http://') or image_data.startswith('https://'):
            print(f"üì• ƒêang t·∫£i ·∫£nh t·ª´ URL: {image_data[:50]}...")
            response = requests.get(image_data, timeout=15)
            if response.status_code == 200:
                base64_data = base64.b64encode(response.content).decode('utf-8')
                mime_type = response.headers.get('Content-Type', 'image/jpeg')
                print(f"‚úÖ ƒê√£ t·∫£i ·∫£nh th√†nh c√¥ng, MIME type: {mime_type}")
                return base64_data, mime_type
            else:
                print(f"‚ö†Ô∏è L·ªói t·∫£i ·∫£nh: HTTP {response.status_code}")
        
        # N·∫øu l√† base64 string v·ªõi data URL
        elif image_data.startswith('data:image'):
            match = re.match(r'data:([^;]+);base64,(.+)', image_data)
            if match:
                mime_type = match.group(1)
                base64_data = match.group(2)
                print(f"‚úÖ ƒê√£ parse data URL, MIME type: {mime_type}")
                return base64_data, mime_type
        
        # N·∫øu l√† raw base64 (kh√¥ng c√≥ prefix)
        else:
            print("‚úÖ S·ª≠ d·ª•ng raw base64 data")
            return image_data, "image/jpeg"
        
        return None, None
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói prepare_image_data: {str(e)}")
        return None, None


def safe_extract_text_from_groq_response(response_data: dict):
    """
    Tr√≠ch xu·∫•t text t·ª´ response Groq m·ªôt c√°ch an to√†n
    """
    try:
        if not response_data:
            return None
        
        # Ki·ªÉm tra error
        if "error" in response_data:
            error = response_data["error"]
            error_msg = error.get('message', 'Unknown error')
            error_type = error.get('type', 'unknown')
            print(f"‚ö†Ô∏è Groq API error [{error_type}]: {error_msg}")
            return None
        
        # L·∫•y content t·ª´ choices
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            
            # Ki·ªÉm tra finish_reason
            finish_reason = choice.get("finish_reason")
            if finish_reason:
                print(f"‚ÑπÔ∏è Finish reason: {finish_reason}")
            
            if finish_reason and finish_reason not in ["stop", "length"]:
                print(f"‚ö†Ô∏è Unusual finish_reason: {finish_reason}")
            
            # L·∫•y text
            if "message" in choice and "content" in choice["message"]:
                text = choice["message"]["content"].strip()
                if text:
                    print(f"‚úÖ Extracted text: '{text}'")
                    return text
        
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y content trong response")
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error parsing Groq response: {e}")
        return None


def clean_detected_text(text: str) -> str:
    """
    L√†m s·∫°ch text t·ª´ AI response - version n√¢ng cao
    """
    if not text:
        return ""
    
    original_text = text
    
    # L√†m s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = text.replace('"', '').replace('*', '').replace('`', '').replace('[', '').replace(']', '').strip()
    
    # X·ª≠ l√Ω c√°c format c√≥ th·ªÉ c√≥
    if ":" in text:
        text = text.split(":")[-1].strip()
    if "\n" in text:
        text = text.split("\n")[0].strip()
    
    # Lo·∫°i b·ªè c√°c t·ª´ th·ª´a (expanded list)
    stop_words = [
        "output", "result", "product", "m√≥n", "l√†", "is", "answer", ":", 
        "t√™n", "s·∫£n ph·∫©m", "ƒë√°p √°n", "the", "this is", "it is",
        "looks like", "appears to be", "seems to be", "probably"
    ]
    
    text_lower = text.lower()
    for word in stop_words:
        if text_lower.startswith(word):
            text = text[len(word):].strip()
            text_lower = text.lower()
    
    # Lo·∫°i b·ªè d·∫•u ch·∫•m c√¢u cu·ªëi
    text = text.rstrip('.,;:!?')
    
    if text != original_text:
        print(f"üßπ Cleaned: '{original_text}' ‚Üí '{text}'")
    
    return text


# ==================== MAIN FUNCTION ====================

def groq_search_product_by_image(image_data: str):
    """
    T√¨m s·∫£n ph·∫©m b·∫±ng h√¨nh ·∫£nh s·ª≠ d·ª•ng Groq Llama 4 Scout Vision API
    
    Args:
        image_data: URL ·∫£nh, base64 string, ho·∫∑c data URL
    
    Returns:
        str: T√™n s·∫£n ph·∫©m t√¨m ƒë∆∞·ª£c, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    print("\n" + "="*70)
    print("üöÄ GROQ LLAMA 4 SCOUT VISION - PRODUCT SEARCH")
    print("="*70)
    
    # B∆∞·ªõc 1: L·∫•y danh s√°ch s·∫£n ph·∫©m
    print("\nüì¶ [1/7] ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ Supabase...")
    products = fetch_product_names()
    
    if not products:
        print("‚ùå Danh s√°ch s·∫£n ph·∫©m r·ªóng")
        return None
    
    print(f"‚úÖ ƒê√£ load {len(products)} s·∫£n ph·∫©m")
    
    if not GROQ_SEARCH_IMAGE_API_KEY:
        print("‚ùå Thi·∫øu GROQ_SEARCH_IMAGE_API_KEY trong .env")
        return None
    
    # B∆∞·ªõc 2: Chu·∫©n b·ªã image data
    print("\nüñºÔ∏è [2/7] ƒêang x·ª≠ l√Ω image data...")
    base64_image, mime_type = prepare_image_data(image_data)
    
    if not base64_image:
        print("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω image data")
        return None
    
    # B∆∞·ªõc 3: T·∫°o prompt t·ªëi ∆∞u cho Llama 4 Scout
    print("\n‚úçÔ∏è [3/7] ƒêang t·∫°o prompt...")
    
    # Chia nh·ªè danh s√°ch n·∫øu qu√° d√†i (tr√°nh v∆∞·ª£t token limit)
    if len(products) > 100:
        print(f"‚ö†Ô∏è Danh s√°ch s·∫£n ph·∫©m l·ªõn ({len(products)} items), s·ª≠ d·ª•ng format t·ªëi ∆∞u")
        product_list = "\n".join([f"{i+1}. {p}" for i, p in enumerate(products[:100])])
        product_list += f"\n... and {len(products)-100} more items"
    else:
        product_list = "\n".join([f"‚Ä¢ {p}" for p in products])
    
    prompt = f"""You are a highly accurate product recognition AI. Analyze the image and identify the product.

PRODUCT DATABASE:
{product_list}

TASK:
1. Carefully examine the image
2. Identify the main object/product
3. Match it to the MOST ACCURATE product name from the list above
4. Return ONLY the exact product name (preserve spelling)

MATCHING RULES:
‚Ä¢ Food/beverages ‚Üí Match to corresponding dish/drink
‚Ä¢ Objects/tools ‚Üí Match to best describing product
‚Ä¢ Electronics ‚Üí Match to similar device
‚Ä¢ Clothing ‚Üí Match to similar apparel
‚Ä¢ Stationery ‚Üí Match to similar item
‚Ä¢ If multiple items visible, focus on the central/main item

OUTPUT FORMAT:
Return ONLY the product name, nothing else. No explanations, no markdown, no extra text.

Example outputs:
C∆°m g√† x·ªëi m·ª°
B√∫n b√≤ Hu·∫ø
Tr√† s·ªØa tr√¢n ch√¢u
√Åo thun basic"""
    
    # B∆∞·ªõc 4: G·ªçi Groq Llama 4 Scout Vision API
    print(f"\nü§ñ [4/7] ƒêang g·ªçi Groq API v·ªõi model: {VISION_MODEL}...")
    
    api_url = "https://api.groq.com/openai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {GROQ_SEARCH_IMAGE_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": VISION_MODEL,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "temperature": 0.05,  # R·∫•t th·∫•p ƒë·ªÉ c√≥ k·∫øt qu·∫£ nh·∫•t qu√°n
        "max_tokens": 200,    # ƒê·ªß cho t√™n s·∫£n ph·∫©m
        "top_p": 0.9,
        "stream": False
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        print(f"üì° Vision API Status: {response.status_code}")
        
        if response.status_code != 200:
            print(f"‚ùå API error {response.status_code}: {response.text[:200]}")
            return None
        
        res = response.json()
        
        # B∆∞·ªõc 5: Tr√≠ch xu·∫•t text
        print("\nüìù [5/7] ƒêang tr√≠ch xu·∫•t k·∫øt qu·∫£...")
        text = safe_extract_text_from_groq_response(res)
        
        if not text:
            print("‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ response")
            return None
        
        # B∆∞·ªõc 6: L√†m s·∫°ch text
        print("\nüßπ [6/7] ƒêang l√†m s·∫°ch output...")
        text = clean_detected_text(text)
        print(f"üéØ Llama 4 Scout detected: '{text}'")
        
        # B∆∞·ªõc 7: Fuzzy matching
        print("\nüîç [7/7] ƒêang so kh·ªõp v·ªõi database...")
        matched_product = fuzzy_match_product(text, products)
        
        if matched_product:
            print("\n" + "="*70)
            print(f"‚úÖ SUCCESS! Found product: '{matched_product}'")
            print("="*70)
            return matched_product
        
        # B∆∞·ªõc 8: Fallback strategy - keyword matching
        print("\n‚ö†Ô∏è Fuzzy matching failed, trying fallback strategy...")
        
        keywords = [
            # ƒê·ªì ƒÉn
            "c∆°m", "ph·ªü", "b√∫n", "b√°nh", "ch·∫£", "g√†", "b√≤", "heo", "t√¥m", "c√°",
            "m√¨", "canh", "l·∫©u", "nem", "g·ªèi", "x√¥i", "ch√°o",
            # ƒê·ªì u·ªëng  
            "tr√†", "c√† ph√™", "n∆∞·ªõc", "sinh t·ªë", "s·ªØa", "bia", "r∆∞·ª£u", "chanh",
            # ƒê·ªì d√πng
            "b√∫t", "v·ªü", "s√°ch", "balo", "t√∫i", "√°o", "qu·∫ßn"
        ]
        
        text_lower = text.lower()
        for keyword in keywords:
            if keyword in text_lower:
                print(f"üîë Found keyword: '{keyword}'")
                for product in products:
                    if keyword in product.lower():
                        print(f"‚ö†Ô∏è Fallback match: '{product}'")
                        return product
        
        print("\n" + "="*70)
        print(f"‚ùå FAILED: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p cho '{text}'")
        print("="*70)
        return None
        
    except requests.exceptions.Timeout:
        print("‚ùå Timeout: API kh√¥ng ph·∫£n h·ªìi trong 30 gi√¢y")
        return None
    
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Request error: {str(e)}")
        return None
    
    except Exception as e:
        print(f"‚ùå Unexpected error: {type(e).__name__} - {str(e)}")
        return None


# ==================== TEST FUNCTION ====================

if __name__ == "__main__":
    print("\n" + "üéØ"*35)
    print("GROQ LLAMA 4 SCOUT VISION - PRODUCT SEARCH TEST")
    print(f"Model: {VISION_MODEL}")
    print("üéØ"*35 + "\n")
    
    # Test 1: URL ·∫£nh
    test_url = "https://example.com/food.jpg"
    print("\n" + "="*70)
    print("TEST 1: Image from URL")
    print("="*70)
    result = groq_search_product_by_image(test_url)
    print(f"\nüìä FINAL RESULT: {result}")
    
    # Test 2: Base64 t·ª´ file local
    print("\n\n" + "="*70)
    print("TEST 2: Image from local file (uncomment to test)")
    print("="*70)
    print("""
    # ƒê·ªÉ test v·ªõi file local:
    import os
    
    image_path = "path/to/your/image.jpg"
    
    if os.path.exists(image_path):
        with open(image_path, "rb") as f:
            base64_data = base64.b64encode(f.read()).decode('utf-8')
            result = groq_search_product_by_image(base64_data)
            print(f"üìä FINAL RESULT: {result}")
    else:
        print(f"‚ö†Ô∏è File not found: {image_path}")
    """)